# 第一周周记 2024.07.07 ~ 2024.07.14

不得不说，每天的时间都过的好快，也许是因为自己真的有很认真的在学习，所以一直没有怎么关注时间。还记得自己在暑假开始之前，还是一个什么也不懂的小白，到现在对神经网络有一定理解，我真的感觉自己改变了很多。我从不会沉下心来学习，每学一段时间就要看一眼手机，到现在手机放在一边也不会碰。从学习不愿意坚持，到现在耐心的学完一个知识点再去探索其他的知识。虽然还有很多值得改进的地方，比如要静下心来钻研、要在学一个东西的时候不要被其他不了解的知识引跑了，因为知识是无穷的，你在学一个知识点的时候，一定有其他的知识点是与其关联的，你不可能强大到将所有知识学完，你能做到的，就是在学这个知识的时候，就把这个知识学精。雷军曾经也说过：“要学会跳读。”希望你在接下来的学习中可以应用到这一点。

讲到机器学习，人工智能这个方向确实是我感兴趣的方向，我也愿意在这个方向不断付出自己的时间和精力。和其他的方向不同，我感觉人工智能是个说起来很高级的，但是你真正去研究的时候并没有那么高级的东西。（也许是因为我学的还不够深）但是这种神奇的内部逻辑和框架，真真正正的吸引到了我，到目前为止，真没有什么方向，是能像人工智能这样，对我有独特魅力的，我不确定我是否以后都会坚持从事这方面的工作，但是我敢保证的是，我现在一定会努力把它学好！！！a's

em，周记要求800字哈，那我就总结一下这周所学吧。
我首先学习了pytorch，pytorch是一个基于python的关于机器学习的库。然后里面封装了很多函数，来减轻代码使用者的编码压力。其中最好用的莫过于dataloader(对数据进行分组)、transform(对数据进行变形)、dataset(建立数据库)以及tensorboard(图形化界面展示)。里面还包含了众多神经网络的实现库，例如：CNN、RNN等，和实现这些神经网络的每一步骤，并且将激活函数、归一化函数、权重更新函数等集成在了一起。

然后是BP神经网络，刚开始学BP的时候，经常容易和BN搞混，现在看来，BP神经网络应该是一切的基础，它的构建方法，前向传播方法（加入激活函数），以及反向传播时用链式法则来求偏导算梯度（RNN和CNN）都被后代所采用。通过隐藏层（将隐藏层视为升维操作）提取数据特征，再通过不同权重的整合，最后输出模型的预测结果。

接着是CNN（卷积神经网络），对于CNN来说，最常见的应用在于图像识别和图像的语义生成，因为CNN的卷积层很好的契合了图像存在高维通道数并且存在多个像素点，不仅如此，池化层的存在也可以将图像的特征进一步广泛化，起到降维、减少参数量、扩大感受野（如果将卷积层、激活函数和池化层看成一个整体）的作用。其中可以关注的点在于1×1卷积核的应用，因为这种类型的卷积核不仅能够起到模型降维的作用，还可以起到升维的作用（通过增删通道数即卷积核个数），在ResNet的残差结构中，便是应用了这种形式，来使通道数和大小达到一致，以实现相加的操作，以获得以前网络结构中留存的信息（类似于LSTM）。现在CNN的赛道无疑就是加深深度或者加长宽度，但是深度和宽度都不能太大，否则会出现梯度消失或参数过大的情况，当然是否可以通过大模型（transformer）的数据即时保存和分批训练能够解决就不是我目前需要的思考的问题了（忽然想到一个问题，如果说神经网络本来就是提取特征，那我是否能够以特征作为目标的输出值进行训练，再用这些特征值对最后的结果做预测，这样就可以减少一次训练的训练量，这种可以分多次，现在的transformer应该也已经应用了）。

我先跳过了MAS（今晚就学），学习了RNN和其套件LSTM。RNN（循环神经网络）由于其独特的使数据前后相关联的性质，在语义分析，文字翻译等NLP（自然语言处理）领域大受欢迎。RNN主要用来预测序列数据，并且它每一步的参数都是共享的，感觉很可以和GNN（图神经网络）联动一下。但是由于其独特的权重共享的机制，导致了它在更深层的层数上会出现权重缺失的情况从而导致求偏导过程中梯度消失或梯度爆炸。所幸后人发明了LSTM和GRU，来使前面层数的关键信息能够以传递到后面的层数上。这里比较有意思的是独热编码这种encoder（编码器），来将词语分布到不同维度上，但是这种方法缺失了不同词语之间的联系，很难表达出复杂语义。

这时，为了应对这种语义关系缺失的情况，Attention机制被发明了。Attention是一种基于RNN的语义分析机制，它能够赋予词语在不同维度上不同的权重，来体现词语的重要程度，并通过欧式距离（N维空间中点与点之间的距离）来判断在当前环境下，其语义之间的关系。其被广泛应用与机器翻译**、**语音识别**、**图像标注等诸多领域。在这里，如果要看Transformer，我觉得很有必要引入一下Encoder-Decoder概念（将现实问题转化为数学问题，通过求解数学问题，从而解决现实问题。），这个概念在transformer中应用的淋漓精致。其主要思想是通过embedding的方式将现实世界中的物品（文字、图片等）转化为维度空间里的向量，这样做的好处，不仅是简化了计算机计算的难度，也可以很好的表现出不同物品之间在高维情况下的关系。其主要运行是编码器和解码器并行启动，并在最后给出结果。


## 区域卷积神经网络（物体检测算法）

[TOC]

### R-CNN：(region 区域)

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407292347590.png)

>  **selective search**：启发性选择**锚框**，在选择了很多锚框之后，在每一个锚框内用预训练好的模型来抽取特征，然后通过**SVM**来对类别进行分类

##### Rol兴趣区域

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407292340279.png)

将锚框内的数据进行切割，然后取每个小格子内的最大值，这样就可以使最后的结果统一化。在进行Rol pooling操作的时候，由于图像和锚框在等比映射后容易出现无法对齐的情况，故往往会进行量化操作（**如下图中，将红色锚框区域量化为橙色区域**），但是这样也舍弃了一些原本正确的数据。

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407300019548.png)



### Fast RCNN：

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407292348537.png)

并不是直接进行**选取锚框**再进行**CNN处理**，而是在进行**CNN卷积操作**抽出特征之后再进行**Rol池化操作**，这样可以简化模型对数据的处理。



### Faster R-CNN：（two stage 两次预测）

当对精度要求高时可以用Faster RCNN

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407292355105.png)

> **RPN**：**Region proposal network**用一个神经网络来替换**selective search**启发性搜索（训练一个二分类来判断锚框是否成功圈中了真实的物体）



### Mask R-CNN：（在无人车领域运用较多）

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407300000680.png)

在faster r-cnn的基础上加入了，对像素进行预测

> **Rol align** 解决了 **RoI pooling**中的数据丢失问题，这里使得数据能够保全，没有丢失。它们之间的区别在于量化，RoI Align在data pooling中**没有使用量化**。这边利用双线性差值选择四个值中最大的作为最终值。

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407300012340.png)

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407300014692.png)

由于Rol align在池化操作时使用到了所有数据，并不会使用量化，故不会丢失数据信息。



### SSD：（单发多框检测）

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407300025290.png)

一次预测，以每个像素为中心产生多个锚框，并直接对锚框内容做预测。通过对不同分辨率的图像设置锚框，可以同时对小物体和大物体做预测

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407300027623.png)

> 速度快但是精度低



### [YoLo V1](http://pjreddie.com/yolo/)：（你只看一次）

**SSD**中锚框大量重叠，因此浪费了许多时间计算，**yolo**则**将锚框均匀分开**，故不会重叠
在核心思想不变的情况下，有不断改进的版本（对于群体性小目标检测效果很差）

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407301207165.png)

> confidence  **Pr(Object)*IOU(truth pred)** Pr为检测物体是否在锚框内（在为1，不在为0） IOU为预测的锚框和实际锚框的交并比

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407301239281.png)

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407301339010.png)

对每个锚框都收集5个信息，最后输出20种查找结果，故最后的网络结果为30层



### YoLo V2：

##### 改进点

1. 在每个layer层后添加了BN层，来对数据进行标准化，并且在使用batch normalization之后可以舍弃掉dropout层
2. 使用了更大的输入尺寸（448×448）

3. 运用了基于anchor的目标检测框的预测方式
4. 对anchor采用了聚类，采用k-means方法来获得priors
5. 采用多尺寸的方式，提高了网络的鲁棒性



### YoLo V3：




## 数据挖掘

[TOC]

### 一、 什么是数据挖掘

从大量数据中提取价值或**“挖掘”**知识，也叫做数据中的知识发现。

将问题并行计算(提高运行效率)

### 二、数据挖掘的具体步骤

许多人把数据挖掘视为“数据中的知识发现”，以下是其具体的步骤：

- **数据清理（消除噪声和不一致数据）**
- **数据集成（不同来源与格式的数据组合到一起）**
- **数据选择（挖掘所需的数据）**
- **数据变换（数据变换成适合挖掘的形式，如汇总，聚集操作）**
- **数据挖掘（方法，建模）**
- **模式评估（结果模型）**
- **知识表示（可视化）**

> 正确率预测  

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407270027312.png)

lit analysis(使用模型和不使用模型做比较)



### 三、数据处理的一些方法

#### 1）聚类算法

##### 1. k-means

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407271137029.png)

> 使用**K-Means++**算法可以更好的将所有智能体收敛到不同的簇中，因为**K-Means++**算法在聚类中心的初始化过程中的基本原则是使得初始的聚类中心之间的相互距离尽可能远（这种由于依靠中心点和中心点之间的有序性进行中心点的划分，虽然避免了初始值敏感问题，可对于特别离散的数据，效果就不是很好了）



##### 2. 二分k-means

**二分k-means**算法是**k-means**算法的改进算法，相比**k-means**算法，它有如下优点：

- **二分k-means**算法可以加速**k-means**算法的执行速度，因为它的相似度计算少了

- 能够克服**k-means**收敛于局部最小的缺点

实际就是把原本设定的K值做为目标条件，而不是前提条件，从整体不断分簇出去形成新的簇。当然也有**AGNES**算法，其原理是将所有智能体视为一个簇，然后通过不断合并簇来实现最后达到K值。



##### 3. Sequential leader clustering(顺序前导聚类)

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407271208156.png)

#### 2）数据预处理

- 同样的数据，从不同角度看，能够得到不同的理解（盲人摸象）
- 需要我们去探究结果的内在联系，才能将他们联系起来
- 谨防幸存者偏差（统计飞机中弹部位，但是已经坠毁的飞机无法被统计）

##### 1. 空值处理（有时有N/A）

ignore空值：当空值占比较小的时候，可以ignore(清除掉)

推测：用领域知识来对空值进行推测

自动处理：通过计算，将平均值或者0填入所有缺失值



##### 2. 异常值与冗余信息检测

离群点分析：根据智能体与其他智能体之间的距离来判断其是否离群

找不同：通过检测有区别的信息，来将可能表示相同的信息放在一起处理



##### 3. 数据采样

由于大数据时代，数据量实在太大了，所以使用采样的方式，来减小我们的一个数据难度，并提取关键特征

不平衡数据：数据分类没有意义，某个范围的值太大了，所以不能准确进行分类

向上采样：将分类的数据量少的部分，进行采样，用不直接复制的方式增大数据量



##### 4. 数据标准化

[归一化和标准化的目的和作用](C:\Users\24468\Desktop\QG人工智能组\Python学习\Python笔记\归一化和标准化的目的和作用.md)



##### 5. 数据描述和数据可视化

更清晰的展示数据的内在逻辑，以及更好的展示数据，一目了然



##### 6. 特征选择

类似于数据采样，选择合适的特征，利用熵（表示随机变量不确定性的度量）来判断预测能否准确

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407271737810.png)



##### 7. 主成分分析(PCA)

**PCA（Principal Component Analysis）**是一种常用的数据分析方法。**PCA**通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。

实际机器学习中处理成千上万甚至几十万维的情况也并不罕见，在这种情况下，机器学习的资源消耗是不可接受的，因此我们必须对数据进行降维。降维当然意味着信息的丢失，不过鉴于实际数据本身常常存在的相关性，我们可以想办法在降维的同时将信息的损失尽量降低。

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407281035538.png)

最简单的降维无疑是通过数据采样和特征选择来减少数据占用的维度，而PCA则是通过坐标变换的方式，对数据进行投影，来使数据在低维度上也能保有一定特征。



##### 8. 线性判别分析(LDA)

PCA是一种无监督的数据降维方法，与之不同的是LDA是一种有监督的数据降维方法。

LDA需要寻找一条合适的直线$y = w^Tx$，使得数据集中的样例投影到该直线时同类样例的投影点尽可能接近，**不同类样例的投影点尽可能远离**，目的是使降维后的数据仍然可分

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407301613457.png)

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407301602940.png)

> 通过Fisher的方式来找到最适合的映射坐标，公式中的分母为**类内散度值**，分子为**类间散度值**，类内散度值体现了映射后的组内值的离散程度，类间散度值体现了不同的簇的中心点在映射后的距离差（最后要求是分母尽可能小，分子尽可能大）

[三个散度矩阵](https://blog.csdn.net/weixin_38313518/article/details/76623744)（看上去很难，其实很简单）

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407301623522.png)

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407301632778.png)

在数学公式的求解下，W最后的值为：

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407301634080.png)

```python
import numpy as np

class LDA(object):
    """
    线性判别分析
    """
    def __init__(self,data,target,d) -> None:
        self.data = data
        self.target = target
        self.d = d
        self.labels = set(target)
        self.mu = self.data.mean(axis=0)
    
    def divide(self):
        """
        功能：将传入的数据集按target分成不同的类别集合并求出对应集合的均值向量
        """
        self.classify,self.classmu = {},{}
        for label in self.labels:
            self.classify[label] = self.data[self.target==label]
            self.classmu[label] = self.classify[label].mean(axis=0)
    
    def getSt(self):
        """
        功能：计算全局散度矩阵
        """
        self.St = np.dot((self.data-self.mu).T,(self.data-self.mu))

    def getSb(self):
        """
        功能：计算类内散度矩阵
        """
        self.Sb = np.zeros((self.data.shape[1],self.data.shape[1]))
        for i in self.labels:
            #获取类别i样例的集合
            classi = self.classify[i]
            #获取类别i的均值向量
            mui = self.classmu[i]
            self.Sb += len(classi) * np.dot((mui - self.mu).reshape(-1,1),(mui - self.mu).reshape(1,-1))

    def getW(self):
        """
        功能：计算w
        """
        self.divide()
        self.getSt()
        self.getSb()
        #St = Sw + Sb
        self.Sw = self.St - self.Sb 
        #计算Sw-1*Sb的特征值和特征向量
        #eig_vectors[:i]与 eig_values相对应
        eig_values, eig_vectors = np.linalg.eig(np.linalg.inv(self.Sw).dot(self.Sb))
        #寻找d个最大非零广义特征值
        topd = (np.argsort(eig_values)[::-1])[:self.d]
        #用d个最大非零广义特征值组成的向量组成w
        self.w = eig_vectors[:,topd]
        

if __name__ == "__main__":
    x = np.array([[1,2,3],[2,1,3],[2,4,1],[1,3,2],[3,6,4],[3,1,1]])
    y = np.array([0,1,2,0,1,2])
    lda = LDA(x,y,2)
    lda.getW()
```


## 数据挖掘

[TOC]

### 一、 什么是数据挖掘

从大量数据中提取价值或**“挖掘”**知识，也叫做数据中的知识发现。

将问题并行计算(提高运行效率)

### 二、数据挖掘的具体步骤

许多人把数据挖掘视为“数据中的知识发现”，以下是其具体的步骤：

- **数据清理（消除噪声和不一致数据）**
- **数据集成（不同来源与格式的数据组合到一起）**
- **数据选择（挖掘所需的数据）**
- **数据变换（数据变换成适合挖掘的形式，如汇总，聚集操作）**
- **数据挖掘（方法，建模）**
- **模式评估（结果模型）**
- **知识表示（可视化）**

> 正确率预测  

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407270027312.png)

lit analysis(使用模型和不使用模型做比较)



### 三、数据处理的一些方法

#### 1）聚类算法

##### 1. k-means

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407271137029.png)

> 使用**K-Means++**算法可以更好的将所有智能体收敛到不同的簇中，因为**K-Means++**算法在聚类中心的初始化过程中的基本原则是使得初始的聚类中心之间的相互距离尽可能远（这种由于依靠中心点和中心点之间的有序性进行中心点的划分，虽然避免了初始值敏感问题，可对于特别离散的数据，效果就不是很好了）



##### 2. 二分k-means

**二分k-means**算法是**k-means**算法的改进算法，相比**k-means**算法，它有如下优点：

- **二分k-means**算法可以加速**k-means**算法的执行速度，因为它的相似度计算少了

- 能够克服**k-means**收敛于局部最小的缺点

实际就是把原本设定的K值做为目标条件，而不是前提条件，从整体不断分簇出去形成新的簇。当然也有**AGNES**算法，其原理是将所有智能体视为一个簇，然后通过不断合并簇来实现最后达到K值。



##### 3. Sequential leader clustering(顺序前导聚类)

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407271208156.png)

#### 2）数据预处理

- 同样的数据，从不同角度看，能够得到不同的理解（盲人摸象）
- 需要我们去探究结果的内在联系，才能将他们联系起来
- 谨防幸存者偏差（统计飞机中弹部位，但是已经坠毁的飞机无法被统计）

##### 1. 空值处理（有时有N/A）

ignore空值：当空值占比较小的时候，可以ignore(清除掉)

推测：用领域知识来对空值进行推测

自动处理：通过计算，将平均值或者0填入所有缺失值

##### 2. 异常值与冗余信息检测

离群点分析：根据智能体与其他智能体之间的距离来判断其是否离群

找不同：通过检测有区别的信息，来将可能表示相同的信息放在一起处理

##### 3. 数据采样

由于大数据时代，数据量实在太大了，所以使用采样的方式，来减小我们的一个数据难度，并提取关键特征

不平衡数据：数据分类没有意义，某个范围的值太大了，所以不能准确进行分类

向上采样：将分类的数据量少的部分，进行采样，用不直接复制的方式增大数据量

##### 4. 数据标准化

[归一化和标准化的目的和作用](C:\Users\24468\Desktop\QG人工智能组\Python学习\Python笔记\归一化和标准化的目的和作用.md)

##### 5. 数据描述和数据可视化

更清晰的展示数据的内在逻辑，以及更好的展示数据，一目了然

##### 6. 特征选择

类似于数据采样，选择合适的特征，利用熵（表示随机变量不确定性的度量）来判断预测能否准确

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407271737810.png)

##### 7. 主成分分析(PCA)

**PCA（Principal Component Analysis）**是一种常用的数据分析方法。**PCA**通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。

实际机器学习中处理成千上万甚至几十万维的情况也并不罕见，在这种情况下，机器学习的资源消耗是不可接受的，因此我们必须对数据进行降维。降维当然意味着信息的丢失，不过鉴于实际数据本身常常存在的相关性，我们可以想办法在降维的同时将信息的损失尽量降低。

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407281035538.png)

最简单的降维无疑是通过数据采样和特征选择来减少数据占用的维度，而PCA则是通过坐标变换的方式，对数据进行投影，来使数据在低维度上也能保有一定特征。
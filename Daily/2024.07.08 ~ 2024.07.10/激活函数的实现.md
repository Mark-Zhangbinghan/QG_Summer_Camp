## 激活函数的实现

#### 为什么要用激活函数：

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407131606279.png)

1）加入非线性激励函数后，神经网络就有可能学习到平滑的曲线来分割平面，而不是用复杂的线性组合逼近平滑曲线来分割平面，使神经网络的表示能力更强了，能够更好的拟合目标函数。
2）如果不用激励函数（其实相当于激励函数是 f ( x ) = x ），在这种情况下你每一层节点的输入都是上层输出的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当

```python
# Sigmoid函数
class SigmoidActivator(object):
    def forward(self, weighted_input):
        return 1.0 / (1.0 + np.exp(-weighted_input))
    def backward(self, output):
        return output * (1 - output)

# Tanh函数
class TanhActivator(object):
    def forward(self, weighted_input):
        return 2.0 / (1.0 + np.exp(-2 * weighted_input)) - 1.0
    def backward(self, output):
        return 1 - output * output
```

#### 饱和激活函数：

##### 1）Sigmoid函数

> 当特征相差不明显时， sigmoid 效果比较好。同时，用 sigmoid 和 tanh 作为激活函数时，需要对输入进行规范化，否则激活后的值全部都进入平坦区，隐层的输出会全部趋同，丧失原有的特征表达。

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407131555465.png)

##### 2）Tanh函数（比Sigmoid函数要好）

> 当输入数据特征相差明显时，用 tanh 的效果会很好，且在循环过程中会不断扩大特征效果并显示出来。

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407131556686.png)

##### 3）Softmax函数

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407131610310.png)

#### 非饱和激活函数：

##### 1）ReLU函数

>  如果使用 ReLU，那么一定要小心设置 learning rate，而且要注意不要让网络出现很多 “dead” 神经元，现在大部分的卷积神经网络都采用ReLU作为激活函数。

![1720857499014](C:\Users\24468\AppData\Roaming\Typora\typora-user-images\1720857499014.png)

##### 2）PReLU函数

![](https://cdn.jsdelivr.net/gh/Mark-Zhangbinghan/QG_Summer_Camp@main/picture/202407131602346.png)
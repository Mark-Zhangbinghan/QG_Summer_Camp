## 了解大模型

#### 1.1 大模型和小模型有什么区别：

小模型通常指参数较少、层数较浅的模型，它们具有轻量级、高效率、易于部署等优点，适用于数据量较小、计算资源有限的场景，例如移动端应用、嵌入式设备、物联网等。

相比小模型，大模型通常参数较多、层数较深，具有更强的表达能力和更高的准确度，但也需要更多的计算资源和时间来训练和推理，适用于数据量较大、计算资源充足的场景，例如云端计算、高性能计算、人工智能等。

#### 1.2 发展历史

- 1998 年，现代卷积神经网络的基本结构 LeNet-5 诞生，机器学习方法由早期基于浅层机器学习的模型，变为了基于深度学习的模型,为[自然语言生成](https://link.zhihu.com/?target=https%3A//xie.infoq.cn/link%3Ftarget%3Dhttps%3A%2F%2Fwww.zhihu.com%2Fsearch%3Fq%3D%25E8%2587%25AA%25E7%2584%25B6%25E8%25AF%25AD%25E8%25A8%2580%25E7%2594%259F%25E6%2588%2590%26search_source%3DEntity%26hybrid_search_source%3DEntity%26hybrid_search_extra%3D%257B%2522sourceType%2522%253A%2522answer%2522%252C%2522sourceId%2522%253A3271533824%257D)、计算机视觉等领域的深入研究奠定了基础，对后续深度学习框架的迭代及大模型发展具有开创性的意义。

- 2013 年，[自然语言处理模型](https://link.zhihu.com/?target=https%3A//xie.infoq.cn/link%3Ftarget%3Dhttps%3A%2F%2Fwww.zhihu.com%2Fsearch%3Fq%3D%25E8%2587%25AA%25E7%2584%25B6%25E8%25AF%25AD%25E8%25A8%2580%25E5%25A4%2584%25E7%2590%2586%25E6%25A8%25A1%25E5%259E%258B%26search_source%3DEntity%26hybrid_search_source%3DEntity%26hybrid_search_extra%3D%257B%2522sourceType%2522%253A%2522answer%2522%252C%2522sourceId%2522%253A3271533824%257D) Word2Vec 诞生，首次提出将单词转换为向量的“词[向量模型](https://link.zhihu.com/?target=https%3A//xie.infoq.cn/link%3Ftarget%3Dhttps%3A%2F%2Fwww.zhihu.com%2Fsearch%3Fq%3D%25E5%2590%2591%25E9%2587%258F%25E6%25A8%25A1%25E5%259E%258B%26search_source%3DEntity%26hybrid_search_source%3DEntity%26hybrid_search_extra%3D%257B%2522sourceType%2522%253A%2522answer%2522%252C%2522sourceId%2522%253A3271533824%257D)”，以便计算机更好地理解和处理文本数据。
- 2014 年，被誉为 21 世纪最强大算法模型之一的 GAN（[对抗式生成网络](https://link.zhihu.com/?target=https%3A//xie.infoq.cn/link%3Ftarget%3Dhttps%3A%2F%2Fwww.zhihu.com%2Fsearch%3Fq%3D%25E5%25AF%25B9%25E6%258A%2597%25E5%25BC%258F%25E7%2594%259F%25E6%2588%2590%25E7%25BD%2591%25E7%25BB%259C%26search_source%3DEntity%26hybrid_search_source%3DEntity%26hybrid_search_extra%3D%257B%2522sourceType%2522%253A%2522answer%2522%252C%2522sourceId%2522%253A3271533824%257D)）诞生，标志着深度学习进入了[生成模型](https://link.zhihu.com/?target=https%3A//xie.infoq.cn/link%3Ftarget%3Dhttps%3A%2F%2Fwww.zhihu.com%2Fsearch%3Fq%3D%25E7%2594%259F%25E6%2588%2590%25E6%25A8%25A1%25E5%259E%258B%26search_source%3DEntity%26hybrid_search_source%3DEntity%26hybrid_search_extra%3D%257B%2522sourceType%2522%253A%2522answer%2522%252C%2522sourceId%2522%253A3271533824%257D)研究的新阶段。

- 2017 年，Google 颠覆性地提出了基于自[注意力机制](https://link.zhihu.com/?target=https%3A//xie.infoq.cn/link%3Ftarget%3Dhttps%3A%2F%2Fwww.zhihu.com%2Fsearch%3Fq%3D%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6%26search_source%3DEntity%26hybrid_search_source%3DEntity%26hybrid_search_extra%3D%257B%2522sourceType%2522%253A%2522answer%2522%252C%2522sourceId%2522%253A3271533824%257D)的神经网络结构——Transformer 架构，奠定了大模型预训练算法架构的基础。
-  2020 年，OpenAI 公司推出了[GPT-3](https://link.zhihu.com/?target=https%3A//xie.infoq.cn/link%3Ftarget%3Dhttps%3A%2F%2Fwww.zhihu.com%2Fsearch%3Fq%3DGPT-3%26search_source%3DEntity%26hybrid_search_source%3DEntity%26hybrid_search_extra%3D%257B%2522sourceType%2522%253A%2522answer%2522%252C%2522sourceId%2522%253A3271533824%257D)，模型参数规模达到了 1750 亿，成为当时最大的语言模型，并且在零样本学习任务上实现了巨大性能提升。随后，更多策略如基于人类反馈的强化学习（RHLF）、代码预训练、[指令微调](https://link.zhihu.com/?target=https%3A//xie.infoq.cn/link%3Ftarget%3Dhttps%3A%2F%2Fwww.zhihu.com%2Fsearch%3Fq%3D%25E6%258C%2587%25E4%25BB%25A4%25E5%25BE%25AE%25E8%25B0%2583%26search_source%3DEntity%26hybrid_search_source%3DEntity%26hybrid_search_extra%3D%257B%2522sourceType%2522%253A%2522answer%2522%252C%2522sourceId%2522%253A3271533824%257D)等开始出现, 被用于进一步提高推理能力和任务泛化。

#### 1.3 大模型的特点

###### · 巨大的规模:

大模型包含数十亿个参数，模型大小可以达到数百 GB 甚至更大。巨大的模型规模使大模型具有强大的表达能力和学习能力。

###### · 涌现能力：

涌现（英语：emergence）或称创发、突现、呈展、演生，是一种现象，为许多小实体相互作用后产生了大实体，而这个大实体展现了组成它的小实体所不具有的特性。引申到模型层面，涌现能力指的是当模型的训练数据突破一定规模，模型突然涌现出之前小模型所没有的、意料之外的、能够综合分析和解决更深层次问题的复杂能力和特性，展现出类似人类的思维和智能。涌现能力也是大模型最显著的特点之一。

###### · 更好的性能和泛化能力：

大模型通常具有更强大的学习能力和泛化能力，能够在各种任务上表现出色，包括自然语言处理、图像识别、语音识别等。

###### · 多任务学习: 

大模型通常会一起学习多种不同的 NLP 任务,如机器翻译、文本摘要、问答系统等。这可以使模型学习到更广泛和泛化的语言理解能力。

###### · 大数据训练: 

大模型需要海量的数据来训练,通常在 TB 以上甚至 PB 级别的数据集。只有大量的数据才能发挥大模型的参数规模优势。

###### · 强大的计算资源: 

训练大模型通常需要数百甚至上千个 GPU,以及大量的时间,通常在几周到几个月。

###### · 迁移学习和预训练： 

大模型可以通过在大规模数据上进行预训练，然后在特定任务上进行微调，从而提高模型在新任务上的性能。

###### · 自监督学习： 

大模型可以通过自监督学习在大规模未标记数据上进行训练，从而减少对标记数据的依赖，提高模型的效能。

###### · 领域知识融合： 

大模型可以从多个领域的数据中学习知识，并在不同领域中进行应用，促进跨领域的创新。

###### · 自动化和效率：

大模型可以自动化许多复杂的任务，提高工作效率，如自动编程、自动翻译、自动摘要等。

